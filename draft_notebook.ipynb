{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install yfinance\n",
    "%pip install scipy\n",
    "%pip install seaborn\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = dt.date(2010,1,1)\n",
    "end_date = dt.date(2023, 12,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = yf.download(\"SPY\", start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlighted stylized facts on the financial asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_adj_close = sp['Adj Close']\n",
    "spy_ret = np.log(spy_adj_close).diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot return series to show volatility clustering\n",
    "spy_ret.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdf graphs to show fat tail\n",
    "ref_mean = spy_ret.mean()\n",
    "ref_std = spy_ret.std()\n",
    "\n",
    "norm_reference = np.random.normal(loc=ref_mean, scale=ref_std, size=spy_ret.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({'Normal Reference': norm_reference, 'SPY Historical Returns': spy_ret}).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(plot_df['SPY Historical Returns'], label='SPY')\n",
    "plt.hist(plot_df['Normal Reference'], label='Normal Ref')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fs conditions for Swing Trader Agent - Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporarily we will use a fixed value 5 for the lookback days\n",
    "shift_returns = spy_ret.shift(1) # make decision based on T-1 day to prevent lookahead bias\n",
    "swing_return_lookback_days = 5\n",
    "Fs = shift_returns - shift_returns.shift(swing_return_lookback_days)\n",
    "\n",
    "# if Fs >0 D will follow like a momentum trader\n",
    "# if Fs <0 D will revert to normal and go against the trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Trader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std = spy_ret.std()/ 2 # this parameters will be trained in the machine learning step\n",
    "\n",
    "def one_noise_demand(std):\n",
    "    return std * np.random.standard_normal()\n",
    "\n",
    "demand_noise_sample = one_noise_demand(noise_std)\n",
    "display(demand_noise_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Trader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewma_alpha = 0.05 # fixed parameters, to be tuned in due course\n",
    "ewm_sr = spy_ret.ewm(alpha=ewma_alpha, adjust=False).mean()\n",
    "\n",
    "ewm_lambda = 4 # we will train the Beta parameters for the momentum trader demands, so we choose lambda = 1 for simplicity\n",
    "\n",
    "momentum_beta = 0.5 # training parameters\n",
    "def one_momentum_trader(beta, ewm_t):\n",
    "    return beta * np.tanh(ewm_t)\n",
    "\n",
    "\n",
    "demand_momentum_sample = one_momentum_trader(momentum_beta, ewm_lambda * ewm_sr.iloc[1])\n",
    "display(demand_momentum_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental Trader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the bull-bear line (MA250) as a simple rule of thumb for the value of the asset\n",
    "bb_lookback_days = 250\n",
    "fundamental_v = 0.2/100 # training parameters\n",
    "\n",
    "def one_fundamental_trader(v, pt, ma250):\n",
    "    return v * (ma250 - pt)\n",
    "\n",
    "spy_ma250 = spy_adj_close.rolling(bb_lookback_days).mean()\n",
    "display(spy_ma250)\n",
    "\n",
    "demand_fundamental_sample = one_fundamental_trader(fundamental_v, spy_adj_close.iloc[-50], spy_ma250.iloc[-50])\n",
    "display(demand_fundamental_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.DataFrame({\n",
    "    'EWM': ewm_sr,\n",
    "    'Close': spy_adj_close,\n",
    "    'F_Value':  spy_ma250,\n",
    "    'log_ret' :spy_ret,\n",
    "}).dropna()\n",
    "# basically we have use some lookback data (EWM/Fs/MA250) so our actual data for trainings starts from 2011\n",
    "display(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Calibration - Samples universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some random values for the parameters\n",
    "from scipy.stats import qmc\n",
    "sobol_sam = qmc.Sobol(d=3)\n",
    "parameter_universe = sobol_sam.random_base2(m=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample simulation using a randomly selected params\n",
    "import random\n",
    "initialized_samples = random.choices(parameter_universe, k=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration for 1 iteration of generating parameters to loss mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamental_v, momentum_beta, noise_std =initialized_samples[0]\n",
    "fundamental_v, momentum_beta, noise_std \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the time period of the input data\n",
    "# input_data = input_data[:'2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(input_data.head())\n",
    "display(input_data.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy trader functions from above\n",
    "def one_fundamental_trader(v, pt, ma250):\n",
    "    return v * (ma250 - pt)\n",
    "\n",
    "def one_momentum_trader(beta, ewm_t, ewm_lambda = 4):\n",
    "    return beta * np.tanh(ewm_lambda * ewm_t)\n",
    "\n",
    "def one_noise_demand(std):\n",
    "    return std * np.random.standard_normal()\n",
    "\n",
    "# start one round of simulation with the selected params\n",
    "def simulate_price_series():\n",
    "    p_cur = input_data.iloc[0]['Close']\n",
    "    ewm_cur = input_data.iloc[0]['EWM']\n",
    "    ewm_alpha = 0.05\n",
    "    yield input_data.iloc[0].name, p_cur\n",
    "    price_series = [p_cur]\n",
    "    for date, _, _, f_value, _ in input_data[1:].itertuples():\n",
    "        \n",
    "        demand = one_fundamental_trader(fundamental_v, p_cur, f_value)+ one_noise_demand(noise_std)\n",
    "        if len(price_series)> 2:\n",
    "            ewm_next = (1-ewm_alpha) * ewm_cur + ewm_alpha * (np.log(price_series[-1]) - np.log(price_series[-2]))\n",
    "            demand += one_momentum_trader(momentum_beta, ewm_next)\n",
    "            ewm_cur = ewm_next \n",
    "        if len(price_series) >5:\n",
    "            fs = np.log(price_series[-1]) - np.log(price_series[-1-5])\n",
    "            if fs >0:\n",
    "                demand += one_momentum_trader(momentum_beta, ewm_next)\n",
    "            else:\n",
    "                demand += one_fundamental_trader(fundamental_v, p_cur, f_value)\n",
    "        p_next = p_cur+demand\n",
    "        yield date, p_next\n",
    "        price_series.append(p_next)\n",
    "        p_cur = p_next\n",
    "        \n",
    "\n",
    "sr = pd.Series(dict(simulate_price_series()))\n",
    "sim_return_sr = np.log(sr).diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_hill_estimator(ret_sr, threshold = 100):\n",
    "    cleaned_sr = ret_sr.copy().replace(0, np.nan).dropna().abs() # ignore 0 ret days and take absolute\n",
    "    ysort = np.sort(cleaned_sr)\n",
    "    iota = 1/(np.mean(np.log(ysort[0:threshold]/ysort[threshold]))) # get the tail index\n",
    "    return iota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_ret_sr = input_data.log_ret[1:]\n",
    "historical_hill = absolute_hill_estimator(hist_ret_sr)\n",
    "sim_hill = absolute_hill_estimator(sim_return_sr)\n",
    "d_hill = abs(sim_hill - historical_hill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_vol = sim_return_sr.std()\n",
    "hist_vol = hist_ret_sr.std()\n",
    "d_vol = abs(sim_vol - hist_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [1,2,3, 10, 11,12, 30, 31, 32]\n",
    "autocorr_diff_sum = sum([abs(sim_return_sr.autocorr(l) - hist_ret_sr.autocorr(l)) for l in lags])\n",
    "d_auto_corr = autocorr_diff_sum/ len(lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqaured_auto_corr_diff_sum = sum([abs(np.square(sim_return_sr).autocorr(l) - np.square(hist_ret_sr).autocorr(l)) for l in lags])\n",
    "d_auto_corr_squared = sqaured_auto_corr_diff_sum/ len(lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = d_hill + d_vol + d_auto_corr + d_auto_corr_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_loss_function(hist_ret, sim_ret):\n",
    "    historical_hill = absolute_hill_estimator(hist_ret)\n",
    "    sim_hill = absolute_hill_estimator(sim_ret)\n",
    "    d_hill = abs(sim_hill - historical_hill)\n",
    "\n",
    "    sim_vol = sim_ret.std()\n",
    "    hist_vol = hist_ret.std()\n",
    "    d_vol = abs(sim_vol - hist_vol)\n",
    "\n",
    "    lags = [1,2,3, 10, 11,12, 30, 31, 32]\n",
    "    autocorr_diff_sum = sum([abs(sim_ret.autocorr(l) - hist_ret.autocorr(l)) for l in lags])\n",
    "    d_auto_corr = autocorr_diff_sum/ len(lags)\n",
    "\n",
    "    sqaured_auto_corr_diff_sum = sum([abs(np.square(sim_ret).autocorr(l) - np.square(hist_ret).autocorr(l)) for l in lags])\n",
    "    d_auto_corr_squared = sqaured_auto_corr_diff_sum/ len(lags)\n",
    "    return d_hill + d_vol + d_auto_corr + d_auto_corr_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_training_data = pd.DataFrame({'fundamental_v': fundamental_v, 'momentum_beta': momentum_beta, 'noise_std':noise_std , 'loss': loss}, index=[0])\n",
    "surrogate_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows how 1 iteration of params => loss will be generated for training the surrogate model, now we will create 10_000 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating all mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_ret_sr = input_data.log_ret[1:]\n",
    "training_set_1 = {(v,beta,std) for v,beta,std in initialized_samples}\n",
    "def generate_trainings(train_data):\n",
    "    for idx, (fundamental_v, momentum_beta, noise_std) in enumerate(train_data):\n",
    "        def simulate_price_series():\n",
    "            p_cur = input_data.iloc[0]['Close']\n",
    "            ewm_cur = input_data.iloc[0]['EWM']\n",
    "            ewm_alpha = 0.05\n",
    "            yield input_data.iloc[0].name, p_cur\n",
    "            price_series = [p_cur]\n",
    "            for date, _, _, f_value, _ in input_data[1:].itertuples():\n",
    "                demand = one_fundamental_trader(fundamental_v, p_cur, f_value) + one_noise_demand(noise_std)\n",
    "                if len(price_series)> 2:\n",
    "                    ewm_next = (1-ewm_alpha) * ewm_cur + ewm_alpha * (np.log(price_series[-1]) - np.log(price_series[-2]))\n",
    "                    demand += one_momentum_trader(momentum_beta, ewm_next) \n",
    "                    ewm_cur = ewm_next\n",
    "                if len(price_series) >5:\n",
    "                    fs = np.log(price_series[-1]) - np.log(price_series[-1-5])\n",
    "                    if fs >0:\n",
    "                        demand += one_momentum_trader(momentum_beta, ewm_next)\n",
    "                    else:\n",
    "                        demand += one_fundamental_trader(fundamental_v, p_cur, f_value)\n",
    "                p_next = p_cur+demand\n",
    "                yield date, p_next\n",
    "                price_series.append(p_next)\n",
    "                p_cur = p_next\n",
    "        \n",
    "        sr = pd.Series(dict(simulate_price_series()))\n",
    "        sim_return_sr = np.log(sr).diff()\n",
    "        loss = distance_loss_function(hist_ret_sr, sim_return_sr)\n",
    "        print('.', end='\\n' if idx %400 ==0 and idx >0 else '')\n",
    "        yield fundamental_v,   momentum_beta,  noise_std ,   loss\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data_1 = list(generate_trainings(training_set_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_train_data = pd.DataFrame(labelled_data_1, columns=['fundamental_v',   'momentum_beta',  'noise_std' ,   'loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save the initial train data to reduce experiment time\n",
    "# xgboost_train_data.to_csv('xg_train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGB for efficient loss estimation without actually running the demand model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "# xgboost_train_data = pd.read_csv('xg_train_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_train_data['loss'] = xgboost_train_data['loss'].clip(upper=1)\n",
    "xgboost_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor = xgb.XGBRegressor(tree_method=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor.fit(xgboost_train_data[['fundamental_v', 'momentum_beta', 'noise_std']], xgboost_train_data[['loss']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_loss from initialized_samples\n",
    "universe_set = {(a,b,c) for a,b,c in parameter_universe}\n",
    "remaining_set = universe_set - training_set_1\n",
    "out = xgb_regressor.predict(list(remaining_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.DataFrame(remaining_set, columns = ['fundamental_v', 'momentum_beta', 'noise_std'])\n",
    "predict_df['predict_dloss'] = out\n",
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df['predict_dloss'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incrementally add new labelled data by taking the parameter set with the lowest predict_dloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get the lowest 1500 predicted params + 500 randomly selected params for further simulating, and then to fit xgboost again\n",
    "predict_select = 1500\n",
    "random_select = 500\n",
    "\n",
    "next_training_df = predict_df.sort_values('predict_dloss').iloc[:predict_select][['fundamental_v','momentum_beta','noise_std']]\n",
    "random_df = predict_df.iloc[predict_select:].sample(n=random_select, random_state=1)[['fundamental_v','momentum_beta','noise_std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_training_set = set(pd.concat([next_training_df, random_df]).itertuples(index=False, name=None))\n",
    "labelled_data = list(generate_trainings(next_training_set))\n",
    "xgboost_train_data_supp = pd.DataFrame(labelled_data, columns=['fundamental_v',   'momentum_beta',  'noise_std' ,   'loss'])\n",
    "all_xgboost_train_data = pd.concat([xgboost_train_data, xgboost_train_data_supp])\n",
    "all_xgboost_train_data['loss'] = all_xgboost_train_data['loss'].clip(upper=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor.fit(all_xgboost_train_data[['fundamental_v', 'momentum_beta', 'noise_std']], all_xgboost_train_data[['loss']])\n",
    "all_training_set = training_set_1 | next_training_set\n",
    "remaining_set = universe_set - all_training_set\n",
    "out = xgb_regressor.predict(list(remaining_set))\n",
    "predict_df = pd.DataFrame(remaining_set, columns = ['fundamental_v', 'momentum_beta', 'noise_std'])\n",
    "predict_df['predict_dloss'] = out\n",
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df['predict_dloss'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the predict error now:\n",
    "check_output = xgb_regressor.predict(list(all_xgboost_train_data[['fundamental_v', 'momentum_beta', 'noise_std']].itertuples(index=False, name=None)))\n",
    "predict_with_true = all_xgboost_train_data.copy()\n",
    "predict_with_true['predict'] = check_output\n",
    "avg_loss = ((predict_with_true['predict'] - predict_with_true['loss'])/predict_with_true['loss']).mean()\n",
    "print(f'Avg loss after supplemented training {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will repeat the extra training 5 more times\n",
    "\n",
    "def re_train(predict_df, all_train_data, training_set):\n",
    "    next_training_df = predict_df.sort_values('predict_dloss').iloc[:predict_select][['fundamental_v','momentum_beta','noise_std']]\n",
    "    random_df = predict_df.iloc[predict_select:].sample(n=random_select, random_state=1)[['fundamental_v','momentum_beta','noise_std']]\n",
    "    next_training_set = set(pd.concat([next_training_df, random_df]).itertuples(index=False, name=None))\n",
    "    labelled_data = list(generate_trainings(next_training_set))\n",
    "    xgboost_train_data_supp = pd.DataFrame(labelled_data, columns=['fundamental_v',   'momentum_beta',  'noise_std' ,   'loss'])\n",
    "    all_xgboost_train_data = pd.concat([all_train_data, xgboost_train_data_supp])\n",
    "    all_xgboost_train_data['loss'] = all_xgboost_train_data['loss'].clip(upper=1)\n",
    "\n",
    "\n",
    "    xgb_regressor.fit(all_xgboost_train_data[['fundamental_v', 'momentum_beta', 'noise_std']], all_xgboost_train_data[['loss']])\n",
    "    all_training_set = training_set | next_training_set\n",
    "    remaining_set = universe_set - all_training_set\n",
    "    out = xgb_regressor.predict(list(remaining_set))\n",
    "    predict_df = pd.DataFrame(remaining_set, columns = ['fundamental_v', 'momentum_beta', 'noise_std'])\n",
    "    predict_df['predict_dloss'] = out\n",
    "    \n",
    "    # check the predict error now:\n",
    "    check_output = xgb_regressor.predict(list(all_xgboost_train_data[['fundamental_v', 'momentum_beta', 'noise_std']].itertuples(index=False, name=None)))\n",
    "    predict_with_true = all_xgboost_train_data.copy()\n",
    "    predict_with_true['predict'] = check_output\n",
    "    avg_loss = ((predict_with_true['predict'] - predict_with_true['loss'])/predict_with_true['loss']).mean()\n",
    "    print(f'\\nAvg loss after re-training {avg_loss:.4f}')\n",
    "    return predict_df, all_xgboost_train_data, all_training_set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'Re-train XGB repeating the steps, loop ={i+1}')\n",
    "    predict_df,all_xgboost_train_data, all_training_set= re_train(predict_df,all_xgboost_train_data, all_training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb_regressor.fit(all_xgboost_train_data[['fundamental_v', 'momentum_beta', 'noise_std']], all_xgboost_train_data[['loss']])\n",
    "\n",
    "final_loss_pred = final_model.predict(parameter_universe)\n",
    "result_df = pd.DataFrame(parameter_universe, columns=['fundamental_v', 'momentum_beta', 'noise_std'])\n",
    "result_df['predict_dloss'] = final_loss_pred\n",
    "result_df.sort_values('predict_dloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the 20 lowest predicted dloss and get the one with the lowest true loss\n",
    "selected_df = result_df.sort_values('predict_dloss').iloc[:20][['fundamental_v','momentum_beta','noise_std']]\n",
    "selected_labelled_data = list(generate_trainings(set(selected_df.itertuples(index=False, name=None))))\n",
    "final_result_df =pd.DataFrame(selected_labelled_data, columns=['fundamental_v',   'momentum_beta',  'noise_std' ,   'loss'])\n",
    "final_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_v,final_beta,final_std = final_result_df.sort_values('loss').iloc[0][['fundamental_v', 'momentum_beta', 'noise_std']]\n",
    "final_v,final_beta,final_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we use this final params into our ABM model, and see the series produced\n",
    "\n",
    "def simulate_price_series_final(v, beta, std, input_data):\n",
    "    p_cur = input_data.iloc[0]['Close']\n",
    "    ewm_cur = input_data.iloc[0]['EWM']\n",
    "    ewm_alpha = 0.05\n",
    "    yield input_data.iloc[0].name, p_cur\n",
    "    price_series = [p_cur]\n",
    "    for date, _, _, f_value, _ in input_data[1:].itertuples():\n",
    "        demand = one_fundamental_trader(v, p_cur, f_value) +  one_noise_demand(std)\n",
    "        if len(price_series)> 2:\n",
    "            ewm_next = (1-ewm_alpha) * ewm_cur + ewm_alpha * (np.log(price_series[-1]) - np.log(price_series[-2]))\n",
    "            demand += one_momentum_trader(momentum_beta, ewm_next)         \n",
    "            ewm_cur = ewm_next\n",
    "        if len(price_series) >5:\n",
    "            fs = np.log(price_series[-1]) - np.log(price_series[-1-5])\n",
    "            if fs >0:\n",
    "                demand += one_momentum_trader(beta, ewm_next)\n",
    "            else:\n",
    "                demand += one_fundamental_trader(v, p_cur, f_value)\n",
    "        p_next = p_cur+demand\n",
    "        yield date, p_next\n",
    "        price_series.append(p_next)\n",
    "        p_cur = p_next\n",
    "        \n",
    "\n",
    "sim_sr_final = pd.Series(dict(simulate_price_series_final(final_v,final_beta,final_std, input_data)))\n",
    "sim_ret_sr_final = np.log(sr).diff()\n",
    "\n",
    "\n",
    "plot_df = pd.DataFrame({'Hist Price': input_data['Close'], 'Sim Price': sim_sr_final})\n",
    "plot_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ret_sr_final.kurt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ret_sr_final.describe(), hist_ret_sr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_ret_sr.kurt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sm.graphics.tsa.plot_acf(hist_ret_sr.values.squeeze(),lags=50 ,title='Historical Returns Autocorrelation')\n",
    "sm.graphics.tsa.plot_acf(np.square(hist_ret_sr).values.squeeze(),lags=50 ,title='Squared Historical Returns Autocorrelation')\n",
    "\n",
    "sm.graphics.tsa.plot_acf(sim_ret_sr_final.dropna().values.squeeze(),lags=50 ,title='Simulated Returns Autocorrelation')\n",
    "sm.graphics.tsa.plot_acf(np.square(sim_ret_sr_final.dropna()).values.squeeze(),lags=50 ,title='Squared Simulated Returns Autocorrelation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
